# ==============================================================================
# WeaveAI æ™ºèƒ½åˆ†æåŠ©æ‰‹ v2.8 - æœ€ç»ˆç‰ˆï¼šå…¨é¢æµå¼è¾“å‡ºä¸æç¤ºè¯ç»ˆæä¼˜åŒ–
# ==============================================================================

# ==============================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“
# ==============================================================================
import streamlit as st
import pandas as pd
import numpy as np
from stqdm import stqdm
import os
import warnings
import re 
import docx
import time

# AI Agent æ‰€éœ€çš„åº“
from dotenv import load_dotenv
from volcenginesdkarkruntime import Ark

# åœ¨ç¨‹åºæœ€å¼€å§‹åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# (å…³é”®) è§£å†³ KMeans å†…å­˜æ³„æ¼è­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '1'

# åˆ†æåº“
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Input
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from pandas.errors import SettingWithCopyWarning, DtypeWarning

# æŠ‘åˆ¶ç‰¹å®šçš„Pandasè­¦å‘Šï¼Œç¾åŒ–è¾“å‡º
warnings.filterwarnings('ignore', category=SettingWithCopyWarning)
warnings.filterwarnings('ignore', category=DtypeWarning)

import plotly.graph_objects as go
import plotly.express as px

# ==============================================================================
# 2. çŠ¶æ€ç®¡ç†ï¼šåˆå§‹åŒ–åº”ç”¨çš„â€œå¤§è„‘â€ (Session State)
# ==============================================================================
def initialize_session_state():
    if 'profile_created' not in st.session_state: st.session_state.profile_created = False
    if 'profile' not in st.session_state: st.session_state.profile = {}
    if 'current_step' not in st.session_state: st.session_state.current_step = 'æœºä¼šæ´å¯Ÿ'
    if 'market_report_content' not in st.session_state: st.session_state.market_report_content = ""
    if 'report_generating' not in st.session_state: st.session_state.report_generating = False
    if 'validation_data' not in st.session_state: st.session_state.validation_data = {'amazon_df': None, 'reviews_df': None}
    if 'validation_summary' not in st.session_state: st.session_state.validation_summary = None

# ==============================================================================
# 3. AI Agent æ¨¡å— (åœ¨å®é™…é¡¹ç›®ä¸­, è¿™éƒ¨åˆ†åº”ç‹¬ç«‹ä¸º ai_agents.py)
# ==============================================================================
def generate_full_report_stream(ark_client, user_profile):
    """ã€æ ¸å¿ƒä¼˜åŒ–ã€‘ä½¿ç”¨ä¼˜åŒ–åçš„é«˜çº§System Prompt"""
    market = user_profile['target_market']
    categories = user_profile['supply_chain']
    seller = user_profile['seller_type']
    price_range = f"${user_profile['min_price']} - ${user_profile['max_price']}"

    system_prompt = f"""
    ä½ æ˜¯ "WeaveAI" åº”ç”¨å†…çš„ä¸€ä½é«˜çº§æˆ˜ç•¥é¡¾é—®ï¼Œä½ çš„æŠ¥å‘Šæ˜¯ä¸ºä¸€ä½è®¡åˆ’è¿›å…¥'{market}'å¸‚åœºçš„'{seller}'ï¼Œä»–/å¥¹ä¸“æ³¨äº'{categories}'å“ç±»ï¼Œç›®æ ‡å”®ä»·åœ¨'{price_range}'ã€‚ä½ çš„æŠ¥å‘Šå¿…é¡»ä¸“ä¸šã€è¯¦å°½ã€å…·æœ‰å‰ç»æ€§ã€æ•°æ®é©±åŠ¨ï¼Œ
    å¹¶ä½¿ç”¨ç²¾ç¾çš„Markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°ï¼ŒåŒ…å«æ ‡é¢˜ã€åˆ—è¡¨å’ŒåŠ ç²—ç­‰å…ƒç´ ã€‚
    æŠ¥å‘Šå¿…é¡»éµå¾ªä»¥ä¸‹ç»“æ„å’Œè¦æ±‚ï¼š

    **ç¬¬ä¸€é˜¶æ®µï¼šè¾“å‡ºæ€è€ƒè¿‡ç¨‹**
    åœ¨æ­£å¼å¼€å§‹æŠ¥å‘Šå‰ï¼Œä½ å¿…é¡»å…ˆè¾“å‡ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚è¿™éƒ¨åˆ†å†…å®¹å¿…é¡»ä»¥ "æˆ‘éœ€è¦..." æˆ– "é¦–å…ˆ..." å¼€å§‹ï¼Œæ¦‚è¿°ä½ å°†å¦‚ä½•ä¸ºç”¨æˆ·åˆ†æã€‚ä¸è¦ä½¿ç”¨ä»»ä½•Markdownæ ‡é¢˜ã€‚
    
    **é‡è¦æŒ‡ä»¤ 1ï¼š** åœ¨æ€è€ƒè¿‡ç¨‹ç»“æŸåï¼Œä½ å¿…é¡»å¦èµ·ä¸€è¡Œï¼Œå¹¶åªè¾“å‡º `<<<<THINKING_ENDS>>>>` è¿™ä¸ªç‰¹æ®Šæ ‡è®°ã€‚
    
    **é‡è¦æŒ‡ä»¤ 2ï¼š** åœ¨ä¸Šä¸€ä¸ªæ ‡è®°ä¹‹åï¼Œä½ å¿…é¡»ç«‹å³å¦èµ·ä¸€è¡Œå¹¶è¾“å‡º `<<<<REPORT_STARTS>>>>`ï¼Œç„¶åæ‰èƒ½å¼€å§‹ç”Ÿæˆä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼çš„æ­£å¼æŠ¥å‘Šï¼Œä¸­é—´ä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚

    **ç¬¬äºŒé˜¶æ®µï¼šè¾“å‡ºæ­£å¼æŠ¥å‘Š**
    ---
    
    ## æŠ¥å‘Šæ‘˜è¦ (Executive Summary)
    *   åœ¨æ­¤å¤„ç”¨2-3ä¸ªè¦ç‚¹ï¼Œ**åŠ ç²—**æ ¸å¿ƒå…³é”®è¯ï¼Œé«˜åº¦æ¦‚æ‹¬æ•´ä¸ªæŠ¥å‘Šçš„æ ¸å¿ƒå‘ç°å’Œæœ€ç»ˆå»ºè®®ã€‚
    
    ---
    
    ## ğŸ¯ å¸‚åœºæœºé‡æ´å¯Ÿ (Market Opportunities)
    
    ### ä¸€ã€ å®è§‚ç¯å¢ƒåˆ†æ
    1.  **å¸‚åœºæ½œåŠ›ä¸è¶‹åŠ¿**: ç»“åˆ**é‡åŒ–æ•°æ®**è§£é‡Šå¢é•¿ç©ºé—´ (å¿…é¡»æ³¨æ˜æ¥æºå’Œå¹´ä»½)ã€‚
    2.  **æ–‡åŒ–ä¸æ¶ˆè´¹ä¹ æƒ¯**: ã€æ ¸å¿ƒã€‘æ·±å…¥åˆ†æå½“åœ°æ–‡åŒ–ã€èŠ‚å‡æ—¥ã€ç”Ÿæ´»æ–¹å¼å¦‚ä½•å½±å“'{categories}'å“ç±»çš„æ¶ˆè´¹åå¥½ã€‚
    3.  **æ³•å¾‹æ³•è§„ä¸å…³ç¨**: ã€æ ¸å¿ƒã€‘æ˜ç¡®æŒ‡å‡ºè¿›å£é™åˆ¶ã€æ‰€éœ€**å…·ä½“è®¤è¯** (å¦‚CE, RoHS) å’Œå¤§è‡´çš„å…³ç¨ç¨ç‡ã€‚
    
    ### äºŒã€ é«˜æ½œåŠ›ç»†åˆ†å“ç±»æœºä¼šç‚¹
    *   ä½ å¿…é¡»åˆ©ç”¨ç½‘ç»œæœç´¢ï¼Œè¯†åˆ«å‡º2ä¸ªæœ€ç¬¦åˆç”¨æˆ·ç”»åƒçš„ç»†åˆ†æœºä¼šã€‚
    *   å¯¹äºæ¯ä¸€ä¸ªæœºä¼šç‚¹ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿è¿›è¡Œåˆ†æï¼š
    
    #### æœºä¼šç‚¹ 1: [åœ¨æ­¤å¤„å¡«å†™å…·ä½“å“ç±»åç§°]
    *   **äº§å“å®šä¹‰:** æ¸…æ™°æè¿°è¿™ä¸ªå“ç±»çš„æ ¸å¿ƒåŠŸèƒ½ã€å½¢æ€å’Œç›®æ ‡ç”¨æˆ·ã€‚
    *   **éœ€æ±‚é©±åŠ¨ä¸å¸‚åœºè§„æ¨¡:** è§£é‡Šä¸ºä»€ä¹ˆå½“åœ°å¸‚åœºéœ€è¦è¿™ä¸ªäº§å“ã€‚**å¿…é¡»åŒ…å«é‡åŒ–æ•°æ®ï¼Œå¹¶æ³¨æ˜æ¥æºå’Œå¹´ä»½** (ä¾‹å¦‚: å¸‚åœºè§„æ¨¡é¢„è®¡åœ¨2025å¹´è¾¾åˆ° **â‚¬5000ä¸‡**ï¼Œå¹´å¢é•¿ç‡ **15%** [æ¥æº: Statista, 2023])ã€‚
    *   **SWOT åˆ†æ:**
        *   **ä¼˜åŠ¿ (Strength):** 
        *   **åŠ£åŠ¿ (Weakness):** 
        *   **æœºä¼š (Opportunity):** 
        *   **å¨èƒ (Threat):** 

    #### æœºä¼šç‚¹ 2: [åœ¨æ­¤å¤„å¡«å†™å…·ä½“å“ç±»åç§°]
    *   (åŒä¸Šç»“æ„)

    ---
    
    ## âš”ï¸ æ ¸å¿ƒç«äº‰æ ¼å±€ (Competitive Landscape)
    
    *   é’ˆå¯¹ä¸Šé¢è¯†åˆ«å‡ºçš„æ¯ä¸€ä¸ªæœºä¼šç‚¹ï¼Œè¿›è¡Œç‹¬ç«‹çš„ç«äº‰åˆ†æã€‚
    
    ### ç«äº‰åˆ†æ: [æœºä¼šç‚¹1çš„å“ç±»åç§°]
  *   **ä¸»è¦ç«äº‰å¯¹æ‰‹åˆ†æ:** ä½ çš„è¡¨æ ¼å¿…é¡»ä¸¥æ ¼éµå®ˆMarkdownè¯­æ³•ï¼Œ**å¹¶ä¸”è¡¨æ ¼æœ¬èº«å¿…é¡»å¦èµ·æ–°çš„ä¸€è¡Œå¼€å§‹**ï¼Œå…¶å‰åä¸èƒ½æœ‰ä»»ä½•æ–‡å­—ã€‚è¯·å‚è€ƒä»¥ä¸‹å®Œç¾èŒƒä¾‹ï¼š
    
*ä¸»è¦ç«äº‰å¯¹æ‰‹åˆ†æè¡¨*
| ä»£è¡¨æ€§ç«å“å“ç‰Œ | ä¸»æµå®šä»· | æ ¸å¿ƒå–ç‚¹ | ä¸»è¦ç”¨æˆ·ç—›ç‚¹ |
| :--- | :--- | :--- | :--- |
| Anker | â‚¬45-â‚¬60 | GaNæŠ€æœ¯, å¤šå£å¿«å…… | éƒ¨åˆ†å‹å·ä½“ç§¯è¾ƒå¤§ |
| Belkin | â‚¬50-â‚¬75 | è‹¹æœå®˜æ–¹è®¤è¯, è®¾è®¡ç®€çº¦ | æ€§ä»·æ¯”ä¸é«˜ |
| TP-Link | â‚¬40-â‚¬60 | ç¨³å®šä¼ è¾“, å¤šè®¾å¤‡å…¼å®¹ | ç”µæ± ç»­èˆªä¸€èˆ¬ |
-æ ¹æ®å®é™…æƒ…å†µè¡¥å……æ›´å¤šç«å“è¡Œã€‚
-  **ç”¨æˆ·è¯„ä»·æ´å¯Ÿ:** åŸºäºç”¨æˆ·è¯„è®ºï¼Œæ€»ç»“å‡º2-3ä¸ªç”¨æˆ·æœ€å…³æ³¨çš„æ ¸å¿ƒéœ€æ±‚å’Œç—›ç‚¹ã€‚

    *   **ç«äº‰ç­–ç•¥å»ºè®®:** åŸºäºä»¥ä¸Šåˆ†æï¼Œæå‡º1-2æ¡é’ˆå¯¹æ€§çš„ã€å¯æ“ä½œçš„ç«äº‰ç­–ç•¥å»ºè®®ã€‚

    ### ç«äº‰åˆ†æ: [æœºä¼šç‚¹2çš„å“ç±»åç§°]
    *   (åŒä¸Šç»“æ„)

    ---

    ## ğŸ“œ æ•°æ®æ¥æºä¸è¯´æ˜
    *   **æ•°æ®æ¥æº:** åœ¨æ­¤åˆ—å‡ºæœ¬æŠ¥å‘Šå¼•ç”¨çš„ä¸»è¦æ•°æ®æ¥æºç½‘ç«™æˆ–æŠ¥å‘Šåç§°ã€‚
    *   **å…è´£å£°æ˜:** æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€æ•°æ®å’ŒAIæ¨¡å‹åˆ†æç”Ÿæˆï¼Œä»…ä¾›å†³ç­–å‚è€ƒï¼Œä¸æ„æˆæœ€ç»ˆæŠ•èµ„å»ºè®®ã€‚å…·ä½“æ•°æ®è¯·ä»¥å®˜æ–¹å‘å¸ƒä¸ºå‡†ã€‚
    """
    user_input = f"è¯·åŸºäºæˆ‘çš„ç”»åƒï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½å…³äº'{market}'å¸‚åœºçš„æœºä¼šè¯†åˆ«ä¸ç«äº‰åˆ†ææŠ¥å‘Šï¼Œé‡ç‚¹å…³æ³¨'{categories}'å“ç±»ã€‚"

    try:
        request_params = {
            "model": "doubao-seed-1-6-250615",
            "input": [{"role": "system", "content": [{"type": "input_text", "text": system_prompt}]},
                      {"role": "user", "content": [{"type": "input_text", "text": user_input}]}],
            "tools": [{"type": "web_search", "limit": 15}],
            "stream": True
        }
        response = ark_client.responses.create(**request_params)
        for chunk in response:
            delta_content = getattr(chunk, 'delta', None)
            if isinstance(delta_content, str):
                yield delta_content
    except Exception as e:
        yield f"âŒ AI Agentè¯·æ±‚å¤±è´¥: {e}"

def agent_action_planner(ark_client, market_report, validation_summary):
    """ã€ä¼˜åŒ–ã€‘Agent 3: è¡ŒåŠ¨è§„åˆ’å¸ˆ - å¢åŠ æµå¼æ€è€ƒè¿‡ç¨‹"""
    system_prompt = f"""
    ä½ æ˜¯ "WeaveAI" åº”ç”¨å†…çš„ä¸€ä½ç»éªŒä¸°å¯Œçš„ç”µå•†æˆ˜ç•¥é¡¾é—®ï¼Œæ“…é•¿å°†åˆ†æè½¬åŒ–ä¸ºæ¸…æ™°ã€å¯æ‰§è¡Œçš„è¡ŒåŠ¨è®¡åˆ’ã€‚ä½ çš„æŠ¥å‘Šå¿…é¡»ä¸“ä¸šã€ç»“æ„åŒ–ï¼Œå¹¶ä½¿ç”¨ç²¾ç¾çš„Markdownæ ¼å¼ã€‚

    **ç¬¬ä¸€é˜¶æ®µï¼šè¾“å‡ºæ€è€ƒè¿‡ç¨‹**
    åœ¨æ­£å¼å¼€å§‹æŠ¥å‘Šå‰ï¼Œä½ å¿…é¡»å…ˆè¾“å‡ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚è¿™éƒ¨åˆ†å†…å®¹å¿…é¡»ä»¥ "æˆ‘éœ€è¦..." æˆ– "é¦–å…ˆ..." å¼€å§‹ï¼Œæ¦‚è¿°ä½ å°†å¦‚ä½•æ•´åˆå¸‚åœºæŠ¥å‘Šå’Œå†…éƒ¨æ•°æ®ï¼Œå¹¶åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ã€‚ä¸è¦ä½¿ç”¨ä»»ä½•Markdownæ ‡é¢˜ã€‚
    
    **é‡è¦æŒ‡ä»¤ 1ï¼š** åœ¨æ€è€ƒè¿‡ç¨‹ç»“æŸåï¼Œä½ å¿…é¡»å¦èµ·ä¸€è¡Œï¼Œå¹¶åªè¾“å‡º `<<<<THINKING_ENDS>>>>` è¿™ä¸ªç‰¹æ®Šæ ‡è®°ã€‚
    
    **é‡è¦æŒ‡ä»¤ 2ï¼š** åœ¨ä¸Šä¸€ä¸ªæ ‡è®°ä¹‹åï¼Œä½ å¿…é¡»ç«‹å³å¦èµ·ä¸€è¡Œå¹¶è¾“å‡º `<<<<REPORT_STARTS>>>>`ï¼Œç„¶åæ‰èƒ½å¼€å§‹ç”Ÿæˆä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼çš„æ­£å¼æŠ¥å‘Šï¼Œä¸­é—´ä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚

    **ç¬¬äºŒé˜¶æ®µï¼šè¾“å‡ºæ­£å¼æŠ¥å‘Š**
    ---

    ## ğŸ“‹ æ‚¨çš„ä¸“å±è¡ŒåŠ¨è®¡åˆ’

    åŸºäºå¸‚åœºæœºä¼šæ´å¯Ÿä¸å†…éƒ¨æ•°æ®éªŒè¯ï¼Œæˆ‘ä»¬ä¸ºæ‚¨åˆ¶å®šäº†ä»¥ä¸‹ä¸‰é˜¶æ®µè¡ŒåŠ¨è·¯çº¿å›¾ï¼š

    ### ğŸš€ äº§å“å¼€å‘ä¸ä¼˜åŒ– (Product)
    *   **æ ¸å¿ƒç›®æ ‡:** [æ­¤å¤„å¡«å†™äº§å“å±‚é¢çš„æ ¸å¿ƒç›®æ ‡]
    *   **è¡ŒåŠ¨é¡¹:**
        1.  **[è¡ŒåŠ¨é¡¹1]:** [è¯¦ç»†æè¿°ï¼Œå°†å…³é”®åŠ¨è¯å’ŒæŒ‡æ ‡ **åŠ ç²—**]ã€‚
        2.  **[è¡ŒåŠ¨é¡¹2]:** [è¯¦ç»†æè¿°]ã€‚
        - æ ¹æ®å®é™…æƒ…å†µè¡¥å……æ›´å¤šè¡ŒåŠ¨é¡¹ã€‚

    ---

    ### ğŸ“¢ è¥é”€ä¸æ¨å¹¿ (Marketing)
    *   **æ ¸å¿ƒç›®æ ‡:** [æ­¤å¤„å¡«å†™è¥é”€å±‚é¢çš„æ ¸å¿ƒç›®æ ‡]
    *   **è¡ŒåŠ¨é¡¹:**
        1.  **[è¡ŒåŠ¨é¡¹1]:** [è¯¦ç»†æè¿°ï¼Œå°†å…³é”®æ¸ é“å’Œæ´»åŠ¨ **åŠ ç²—**]ã€‚
        2.  **[è¡ŒåŠ¨é¡¹2]:** [è¯¦ç»†æè¿°]ã€‚
        - æ ¹æ®å®é™…æƒ…å†µè¡¥å……æ›´å¤šè¡ŒåŠ¨é¡¹ã€‚
    ---

    ### ğŸ“¦ åº“å­˜ä¸è¿è¥ (Operations)
    *   **æ ¸å¿ƒç›®æ ‡:** [æ­¤å¤„å¡«å†™è¿è¥å±‚é¢çš„æ ¸å¿ƒç›®æ ‡]
    *   **è¡ŒåŠ¨é¡¹:**
        1.  **[è¡ŒåŠ¨é¡¹1]:** [è¯¦ç»†æè¿°ï¼Œå°†å…³é”®æµç¨‹å’Œè®¤è¯ **åŠ ç²—**]ã€‚
        2.  **[è¡ŒåŠ¨é¡¹2]:** [è¯¦ç»†æè¿°]ã€‚
        - æ ¹æ®å®é™…æƒ…å†µè¡¥å……æ›´å¤šè¡ŒåŠ¨é¡¹ã€‚
    """
    user_input = f"ä»¥ä¸‹æ˜¯æˆ‘çš„å†³ç­–ä¾æ®ï¼š\n--- [å¸‚åœºæœºä¼šæŠ¥å‘Š] ---\n{market_report}\n--- [å†…éƒ¨æ•°æ®éªŒè¯æ‘˜è¦] ---\n{validation_summary}\n---\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½å…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’ã€‚"
    try:
        request_params = {"model": "doubao-seed-1-6-250615", "input": [{"role": "system", "content": [{"type": "input_text", "text": system_prompt}]}, {"role": "user", "content": [{"type": "input_text", "text": user_input}]}], "stream": True}
        response = ark_client.responses.create(**request_params)
        for chunk in response:
            delta_content = getattr(chunk, 'delta', None)
            if isinstance(delta_content, str):
                yield delta_content
    except Exception as e:
        yield f"âŒ è¡ŒåŠ¨è§„åˆ’å¸ˆAgentè¯·æ±‚å¤±è´¥: {e}"

def generate_review_summary_report(ark_client, positive_reviews_sample, negative_reviews_sample):
    """ã€ä¼˜åŒ–ã€‘Agent 4: ç”¨æˆ·æ´å¯Ÿåˆ†æå¸ˆ - å¢åŠ æµå¼æ€è€ƒè¿‡ç¨‹"""
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ "WeaveAI" åº”ç”¨å†…çš„é«˜çº§ç”¨æˆ·æ´å¯Ÿåˆ†æå¸ˆï¼Œä¸“æ³¨äºä»ç”¨æˆ·è¯„è®ºä¸­æç‚¼å•†ä¸šæ´è§ã€‚ä½ çš„æŠ¥å‘Šå¿…é¡»ä¸“ä¸šã€ç»“æ„åŒ–ï¼Œå¹¶ä½¿ç”¨ç²¾ç¾çš„Markdownæ ¼å¼ã€‚

    **ç¬¬ä¸€é˜¶æ®µï¼šè¾“å‡ºæ€è€ƒè¿‡ç¨‹**
    åœ¨æ­£å¼å¼€å§‹æŠ¥å‘Šå‰ï¼Œä½ å¿…é¡»å…ˆè¾“å‡ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚è¿™éƒ¨åˆ†å†…å®¹å¿…é¡»ä»¥ "æˆ‘éœ€è¦..." æˆ– "é¦–å…ˆ..." å¼€å§‹ï¼Œæ¦‚è¿°ä½ å°†å¦‚ä½•åˆ†æè¿™äº›è¯„è®ºã€‚ä¸è¦ä½¿ç”¨ä»»ä½•Markdownæ ‡é¢˜ã€‚
    
    **é‡è¦æŒ‡ä»¤ 1ï¼š** åœ¨æ€è€ƒè¿‡ç¨‹ç»“æŸåï¼Œä½ å¿…é¡»å¦èµ·ä¸€è¡Œï¼Œå¹¶åªè¾“å‡º `<<<<THINKING_ENDS>>>>` è¿™ä¸ªç‰¹æ®Šæ ‡è®°ã€‚
    
    **é‡è¦æŒ‡ä»¤ 2ï¼š** åœ¨ä¸Šä¸€ä¸ªæ ‡è®°ä¹‹åï¼Œä½ å¿…é¡»ç«‹å³å¦èµ·ä¸€è¡Œå¹¶è¾“å‡º `<<<<REPORT_STARTS>>>>`ï¼Œç„¶åæ‰èƒ½å¼€å§‹ç”Ÿæˆä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹Markdownæ ¼å¼çš„æ­£å¼æŠ¥å‘Šï¼Œä¸­é—´ä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚

    **ç¬¬äºŒé˜¶æ®µï¼šè¾“å‡ºæ­£å¼æŠ¥å‘Š**
    ---

    ### ğŸ“ è¯„è®ºæ€»ä½“æƒ…ç»ªæ¦‚è¿°
    *   åŸºäºä½ çœ‹åˆ°çš„è¯„è®ºï¼Œç”¨ä¸€ä¸¤å¥è¯æ€»ç»“äº§å“çš„æ•´ä½“å¸‚åœºåå“å’Œç”¨æˆ·æƒ…ç»ªã€‚

    ---

    ### ğŸ‘ äº§å“æ ¸å¿ƒä¼˜åŠ¿ (ä»æ­£é¢è¯„è®ºä¸­æç‚¼)
    *   ä½¿ç”¨åˆ—è¡¨å½¢å¼ï¼Œæ€»ç»“å‡ºç”¨æˆ·æœ€å¸¸ç§°èµçš„2-3ä¸ªæ ¸å¿ƒä¼˜ç‚¹ã€‚
    *   å¯¹äºæ¯ä¸ªä¼˜ç‚¹ï¼Œè¯·ä½¿ç”¨ blockquote æ ¼å¼å¼•ç”¨ä¸€å¥æœ€èƒ½ä»£è¡¨è¯¥è§‚ç‚¹çš„ **åŸå§‹è¯„è®º**ã€‚
    
        > ç¤ºä¾‹ï¼šè¿™æ˜¯ä¸€ä¸ªå¼•ç”¨çš„åŸå§‹è¯„è®ºã€‚

    ---

    ### ğŸ‘ äº§å“ä¸»è¦ç—›ç‚¹ (ä»è´Ÿé¢è¯„è®ºä¸­æç‚¼)
    *   ä½¿ç”¨åˆ—è¡¨å½¢å¼ï¼Œæ€»ç»“å‡ºç”¨æˆ·æŠ±æ€¨æœ€å¤šçš„2-3ä¸ªæ ¸å¿ƒé—®é¢˜æˆ–ç¼ºç‚¹ã€‚
    *   å¯¹äºæ¯ä¸ªç—›ç‚¹ï¼ŒåŒæ ·ä½¿ç”¨ blockquote æ ¼å¼å¼•ç”¨ä¸€å¥æœ€èƒ½ä»£è¡¨è¯¥è§‚ç‚¹çš„ **åŸå§‹è¯„è®º**ã€‚

    ---
    
    ### ğŸ’¡ å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®
    *   åŸºäºä»¥ä¸Šåˆ†æï¼Œä¸ºäº§å“æˆ–è¿è¥å›¢é˜Ÿæä¾›2-3æ¡å…·ä½“çš„ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®ã€‚æ¯ä¸ªå»ºè®®éƒ½åº”è¯¥æ¸…æ™°åœ°è¯´æ˜ **â€œåšä»€ä¹ˆâ€** å’Œ **â€œä¸ºä»€ä¹ˆâ€**ã€‚
    """
    user_input = f"ä»¥ä¸‹æ˜¯å…³äºæŸæ¬¾äº§å“çš„ç”¨æˆ·è¯„è®ºæ ·æœ¬ã€‚\n--- [æ­£é¢è¯„è®ºæ ·æœ¬] ---\n{positive_reviews_sample}\n--- [æ­£é¢è¯„è®ºæ ·æœ¬ç»“æŸ] ---\n--- [è´Ÿé¢è¯„è®ºæ ·æœ¬] ---\n{negative_reviews_sample}\n--- [è´Ÿé¢è¯„è®ºæ ·æœ¬ç»“æŸ] ---\nè¯·æ ¹æ®ä»¥ä¸Šè¯„è®ºï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½ç”¨æˆ·æ´å¯Ÿåˆ†ææŠ¥å‘Šã€‚"
    try:
        request_params = {"model": "doubao-seed-1-6-250615", "input": [{"role": "system", "content": [{"type": "input_text", "text": system_prompt}]}, {"role": "user", "content": [{"type": "input_text", "text": user_input}]}], "stream": True}
        response = ark_client.responses.create(**request_params)
        for chunk in response:
            delta_content = getattr(chunk, 'delta', None)
            if isinstance(delta_content, str):
                yield delta_content
    except Exception as e:
        yield f"âŒ AIè¯„è®ºåˆ†æè¯·æ±‚å¤±è´¥: {e}"

# ==============================================================================
# 4. æ•°æ®å¤„ç†æ¨¡å— (ä¿æŒä¸å˜)
# ==============================================================================
@st.cache_data
def convert_df_to_csv(df): return df.to_csv(index=False).encode('utf-8-sig')

def load_uploaded_file(uploaded_file, dtype_spec=None):
    if uploaded_file is None: return None;
    try:
        file_name = uploaded_file.name
        if file_name.endswith('.parquet'): return pd.read_parquet(uploaded_file)
        elif file_name.endswith('.csv'): return pd.read_csv(uploaded_file, on_bad_lines='skip', dtype=dtype_spec)
        else: st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_name}ã€‚"); return None
    except Exception as e: st.error(f"è¯»å–æ–‡ä»¶ '{uploaded_file.name}' æ—¶å‡ºé”™: {e}"); return None

@st.cache_data
def perform_lstm_forecast(_df):
    sales_ts = _df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0); sales_values = sales_ts.values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1)); scaled_values = scaler.fit_transform(sales_values)
    def create_dataset(data, look_back=7):
        X, y = [], [];
        for i in range(len(data) - look_back): X.append(data[i:(i + look_back), 0]); y.append(data[i + look_back, 0])
        return np.array(X), np.array(y)
    look_back = 7; X, y = create_dataset(scaled_values, look_back); X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)]); model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X, y, epochs=20, batch_size=32, verbose=0)
    last_days_scaled = scaled_values[-look_back:]; current_input = np.reshape(last_days_scaled, (1, look_back, 1)); future_predictions_scaled = []
    for _ in range(30):
        next_pred_scaled = model.predict(current_input, verbose=0); future_predictions_scaled.append(next_pred_scaled[0, 0])
        new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1)); current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)
    future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))
    last_date = sales_ts.index[-1]; future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30); fig = go.Figure()
    fig.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='å†å²é”€å”®é¢', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))
    fig.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM é¢„æµ‹é”€å”®é¢', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))
    fig.update_layout(title='æœªæ¥30å¤©é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTMæ¨¡å‹)', xaxis_title='æ—¥æœŸ', yaxis_title='é”€å”®é¢'); return fig

@st.cache_data
def perform_product_clustering(_df):
    required_cols = ['SKU', 'Amount', 'Qty', 'Order ID'];
    if not all(col in _df.columns for col in _df.columns): return None, None, f"èšç±»åˆ†æå¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚"
    product_agg_df = _df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()
    features = product_agg_df[['total_amount', 'total_qty', 'order_count']]; scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)
    kmeans = KMeans(n_clusters=3, n_init=10, random_state=42); product_agg_df['cluster'] = kmeans.fit_predict(features_scaled)
    cluster_summary = product_agg_df.groupby('cluster')[['total_amount', 'total_qty', 'order_count']].mean().sort_values(by='total_amount', ascending=False)
    hot_product_cluster_id = cluster_summary.index[0]
    hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)
    return cluster_summary, hot_products, None

def find_review_column(df):
    priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']
    for p_col in priority_cols:
        if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col
    possible_cols = [col for col in df.columns if any(key in str(col).lower() for key in ['text', 'review', 'content', 'comment'])]
    if possible_cols:
        string_cols = [col for col in possible_cols if df[col].dtype == 'object'];
        if string_cols: return max(string_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())
    object_cols = df.select_dtypes(include=['object']).columns
    if not object_cols.empty:
        for col in object_cols:
            if df[col].dropna().astype(str).str.strip().any(): return col
    return None
    
@st.cache_data
def perform_sentiment_analysis(reviews_df):
    def sentiment_to_rating(sentiment):
        if sentiment >= 0.5: return 5
        elif sentiment >= 0.05: return 4
        elif sentiment > -0.05: return 3
        elif sentiment > -0.5: return 2
        else: return 1
    analyzer = SentimentIntensityAnalyzer(); review_column_name = find_review_column(reviews_df);
    if review_column_name is None: return None, "é”™è¯¯: æœªèƒ½åœ¨è¯„è®ºæ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬åˆ—ã€‚"
    reviews_df[review_column_name] = reviews_df[review_column_name].astype(str).dropna()
    reviews_df = reviews_df[reviews_df[review_column_name].str.strip() != 'None'].copy()
    stqdm.pandas(desc="æ­£åœ¨è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"); reviews_df['sentiment'] = reviews_df[review_column_name].progress_apply(lambda text: analyzer.polarity_scores(text)['compound'])
    if 'rating' not in reviews_df.columns: reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)
    reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True); return reviews_df, None

@st.cache_data
def create_category_sales_plot(_df):
    category_means = _df.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()
    fig = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', labels={'Category': 'äº§å“ç±»åˆ«', 'Amount': 'å¹³å‡é”€å”®é¢ (Amount)'}, title='å„äº§å“ç±»åˆ«å¹³å‡é”€å”®é¢å¯¹æ¯”')
    fig.update_layout(width=800, height=500, template='plotly_white', showlegend=False)
    fig.update_traces(textposition='outside')
    return fig

# ==============================================================================
# 5. UI æ¸²æŸ“å‡½æ•° (å°†æ¯ä¸ªé¡µé¢çš„UIé€»è¾‘å°è£…)
# ==============================================================================
def render_profile_creation_page():
    """æ¸²æŸ“æˆ˜ç•¥æ¡£æ¡ˆåˆ›å»ºé¡µé¢"""
    st.header("ğŸš€ åˆ›å»ºæ‚¨çš„ä¸“å±æˆ˜ç•¥æ¡£æ¡ˆ"); st.info("è¯·å‘Šè¯‰æˆ‘ä»¬æ‚¨çš„å•†ä¸šç”»åƒï¼ŒAIå°†ä¸ºæ‚¨æä¾›é«˜åº¦å®šåˆ¶åŒ–çš„åˆ†æã€‚")
    with st.form(key='profile_form'):
        st.subheader("1. æ‚¨çš„å•†ä¸šèº«ä»½"); seller_type = st.selectbox("å–å®¶ç±»å‹", ["å“ç‰Œæ–¹", "å·¥å‚è½¬å‹", "è´¸æ˜“å•†", "ä¸ªäººå–å®¶"], help="æ‚¨çš„å•†ä¸šæ¨¡å¼å†³å®šäº†AIå»ºè®®çš„ä¾§é‡ç‚¹ã€‚")
        st.subheader("2. æ‚¨çš„æ ¸å¿ƒä¾›åº”é“¾"); supply_chain = st.text_input("ä¸»è¥/æ„Ÿå…´è¶£å“ç±» (ç”¨é€—å·åˆ†éš”)", "æ¶ˆè´¹ç”µå­, æˆ·å¤–ç”¨å“, å® ç‰©å®¶å±…", help="è¾“å…¥æ‚¨æœ‰ä¼˜åŠ¿æˆ–æ„Ÿå…´è¶£çš„äº§å“å¤§ç±»ã€‚")
        st.subheader("3. æ‚¨çš„ç›®æ ‡å¸‚åœºä¸å®šä»·ç­–ç•¥"); target_market = st.text_input("æ„å‘å¸‚åœº", "å¾·å›½", help="æ‚¨æƒ³è¿›å…¥å“ªä¸ªå›½å®¶æˆ–åœ°åŒºï¼Ÿ"); price_range = st.slider("æœŸæœ›äº§å“å”®ä»·åŒºé—´ ($)", 1, 500, (20, 80), help="è®¾å®šä»·æ ¼èŒƒå›´æœ‰åŠ©äºAIç²¾å‡†æ¨èã€‚")
        if st.form_submit_button(label='âœ”ï¸ ä¿å­˜æ¡£æ¡ˆå¹¶å¼€å§‹åˆ†æ'):
            st.session_state.profile = {"seller_type": seller_type, "supply_chain": supply_chain, "target_market": target_market, "min_price": price_range[0], "max_price": price_range[1]}; st.session_state.profile_created = True; st.rerun()

def render_insight_page():
    """æ¸²æŸ“ç¬¬ä¸€æ­¥ï¼šæœºä¼šæ´å¯Ÿé¡µé¢ï¼Œå®ç°UIçŠ¶æ€åˆ†ç¦»"""
    st.title("ç¬¬ä¸€æ­¥ï¼šæœºä¼šæ´å¯Ÿ (Insight)"); st.markdown("åŸºäºæ‚¨çš„æˆ˜ç•¥æ¡£æ¡ˆï¼ŒAIä¸“å®¶å›¢é˜Ÿå°†ä¸ºæ‚¨åˆ†æç›®æ ‡å¸‚åœºï¼Œè¯†åˆ«é«˜æ½œåŠ›æœºä¼šã€‚")
    if not (api_key_from_env := os.environ.get("ARK_API_KEY")): st.error("æ— æ³•æ‰¾åˆ° API Keyã€‚è¯·ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®ã€‚"); return

    if st.session_state.market_report_content and not st.session_state.report_generating:
        st.markdown("---"); st.header("ğŸ“ˆ æ‚¨çš„åŠ¨æ€æœºä¼šæ¡£æ¡ˆ"); st.markdown(st.session_state.market_report_content)
        st.success("æœºä¼šæ´å¯Ÿå·²å®Œæˆï¼è¯·å‰å¾€ä¸‹ä¸€æ­¥è¿›è¡Œâ€œè‡ªæˆ‘éªŒè¯â€ã€‚")
        if st.button("ğŸ”„ é‡æ–°ç”ŸæˆæŠ¥å‘Š"): st.session_state.market_report_content = ""; st.session_state.report_generating = True; st.rerun()
    elif st.session_state.report_generating:
        st.info("æŠ¥å‘Šç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...")
        expander = st.expander("ç‚¹å‡»æŸ¥çœ‹AIçš„æ€è€ƒè¿‡ç¨‹ ğŸ§ ", expanded=True)
        thinking_placeholder = expander.empty(); report_placeholder = st.empty()
        full_response_text = ""; separator_think_end = "<<<<THINKING_ENDS>>>>"; separator_report_start = "<<<<REPORT_STARTS>>>>"
        try:
            ark_client = Ark(api_key=api_key_from_env)
            for chunk in generate_full_report_stream(ark_client, st.session_state.profile):
                full_response_text += chunk
                if separator_report_start in full_response_text:
                    parts1 = full_response_text.split(separator_think_end, 1); thinking_placeholder.markdown(parts1[0])
                    report_part = parts1[1].split(separator_report_start, 1)[1]; report_placeholder.markdown(report_part)
                elif separator_think_end in full_response_text: thinking_placeholder.markdown(full_response_text.split(separator_think_end, 1)[0])
                else: thinking_placeholder.markdown(full_response_text + "...")
            if separator_report_start in full_response_text: st.session_state.market_report_content = full_response_text.split(separator_report_start, 1)[1].strip()
            else: st.session_state.market_report_content = "AIæœªèƒ½ç”Ÿæˆæ ¼å¼æ­£ç¡®çš„æŠ¥å‘Šï¼Œè¯·é‡è¯•ã€‚"
        except Exception as e: st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"); st.session_state.market_report_content = ""
        st.session_state.report_generating = False; st.success("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼"); time.sleep(1); st.rerun()
    else:
        if st.button("ğŸ¤– è°ƒç”¨AIä¸“å®¶å›¢é˜Ÿç”ŸæˆæŠ¥å‘Š", type="primary", use_container_width=True): st.session_state.report_generating = True; st.rerun()

def render_validation_page():
    """æ¸²æŸ“ç¬¬äºŒæ­¥ï¼šè‡ªæˆ‘éªŒè¯é¡µé¢"""
    st.title("ç¬¬äºŒæ­¥ï¼šè‡ªæˆ‘éªŒè¯ (Validation)"); st.info("ä¸Šä¼ æ‚¨çš„å†å²é”€å”®å’Œè¯„è®ºæ•°æ®ï¼Œçœ‹çœ‹æ‚¨çš„ä¸šåŠ¡ç°çŠ¶ä¸AIå‘ç°çš„æœºä¼šç‚¹æ˜¯å¦åŒ¹é…ã€‚")
    uploaded_amazon = st.file_uploader('1. ä¸Šä¼  Amazon é”€å”®æŠ¥å‘Š', type=['csv', 'parquet'], key='amazon_uploader')
    uploaded_reviews = st.file_uploader('2. ä¸Šä¼  Amazon è¯„è®ºæ•°æ® (å¯é€‰)', type=['csv', 'parquet'], key='reviews_uploader')
    if uploaded_amazon: st.session_state.validation_data['amazon_df'] = load_uploaded_file(uploaded_amazon, {'ASIN': str, 'asin': str})
    if uploaded_reviews: st.session_state.validation_data['reviews_df'] = load_uploaded_file(uploaded_reviews)
    if (amazon_df := st.session_state.validation_data.get('amazon_df')) is not None:
        with st.status("âš™ï¸ æ­£åœ¨æ¸…æ´—å’Œé€‚é…æ‚¨çš„é”€å”®æ•°æ®...", expanded=True) as status:
            try:
                for old, new in {'Total Sales':'Amount','Product':'SKU','Quantity':'Qty','Order_ID':'Order ID'}.items():
                    if old in amazon_df.columns: amazon_df.rename(columns={old:new}, inplace=True)
                if missing := [c for c in ["Amount","Category","Date","Status","SKU","Order ID","Qty"] if c not in amazon_df.columns]: status.update(label="æ•°æ®æ¸…æ´—å¤±è´¥!", state="error"); st.error(f"Amazon æ–‡ä»¶ä¸­ç¼ºå°‘å…³é”®åˆ—: {', '.join(missing)}"); return
                amazon_df.dropna(subset=["Amount", "Category", "Date"], inplace=True)
                try: amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], format='%m-%d-%y')
                except ValueError: amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], errors='coerce')
                amazon_df["Amount"] = pd.to_numeric(amazon_df["Amount"], errors='coerce'); amazon_df = amazon_df[amazon_df["Status"].isin(["Shipped","Shipped - Delivered to Buyer","Completed","Pending","Cancelled"])]; amazon_df.dropna(subset=['Date','Amount','SKU','Order ID','Qty'], inplace=True)
                status.update(label="æ•°æ®æ¸…æ´—ä¸é€‚é…å®Œæˆ!", state="complete", expanded=False)
            except Exception as e: status.update(label="æ•°æ®å¤„ç†å‡ºé”™!", state="error"); st.error(f"æ•°æ®æ¸…æ´—æ—¶å‘ç”Ÿé”™è¯¯: {e}"); return
        st.success("æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼è¯·æµè§ˆä¸‹æ–¹çš„åˆ†æç»“æœã€‚")
        tab_lstm, tab_category, tab_cluster, tab_sentiment = st.tabs(["ğŸ§  LSTMé”€å”®é¢„æµ‹", "ğŸ›ï¸ å“ç±»è¡¨ç°", "ğŸ”¥ çƒ­é”€å“èšç±»", "ğŸ’¬ æƒ…æ„Ÿåˆ†æ"])
        with tab_lstm: st.header("é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTM)"); st.plotly_chart(perform_lstm_forecast(amazon_df), use_container_width=True)
        with tab_category: st.header("äº§å“ç±»åˆ«é”€å”®è¡¨ç°"); st.plotly_chart(create_category_sales_plot(amazon_df))
        with tab_cluster:
            st.header("çƒ­é”€å•†å“èšç±»åˆ†æ")
            cluster_summary, hot_products, cluster_error = perform_product_clustering(amazon_df)
            if cluster_error: st.error(cluster_error)
            elif cluster_summary is not None and hot_products is not None:
                st.subheader("å„å•†å“ç°‡ç‰¹å¾å‡å€¼"); st.dataframe(cluster_summary); st.subheader(f"ğŸ”¥ çƒ­é”€å•†å“åˆ—è¡¨ (å…± {len(hot_products)} ä¸ª)"); st.dataframe(hot_products.head(20))
                st.session_state.validation_summary = f"å†…éƒ¨æ•°æ®æ˜¾ç¤ºï¼Œæˆ‘çš„çƒ­é”€å•†å“ä¸»è¦é›†ä¸­åœ¨ç°‡{cluster_summary.index[0]}ï¼Œå¹³å‡é”€å”®é¢ä¸º{cluster_summary.iloc[0]['total_amount']:.2f}ã€‚ä¸»è¦çƒ­é”€SKUåŒ…æ‹¬ï¼š{', '.join(hot_products['SKU'].head(3).tolist())}ã€‚"
        with tab_sentiment:
            st.header("å®¢æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æ")
            if (reviews_df := st.session_state.validation_data.get('reviews_df')) is not None:
                sentiment_df, sentiment_error = perform_sentiment_analysis(reviews_df)
                if sentiment_error: st.error(sentiment_error)
                elif sentiment_df is not None:
                    st.subheader("æŒ‰æ˜Ÿçº§ç­›é€‰è¯„è®º"); rating_range = st.slider('é€‰æ‹©æ˜Ÿçº§èŒƒå›´:', 1, 5, (1, 5)); min_r, max_r = rating_range
                    filtered_reviews = sentiment_df[(sentiment_df['rating'] >= min_r) & (sentiment_df['rating'] <= max_r)]
                    st.markdown(f"**æ˜¾ç¤º {len(filtered_reviews)} æ¡è¯„åˆ†ä¸º {min_r} åˆ° {max_r} æ˜Ÿçš„è¯„è®º**"); st.dataframe(filtered_reviews[['rating', 'review_text', 'sentiment']])
                    st.subheader("æƒ…æ„Ÿåˆ†æ•°ç»Ÿè®¡"); avg_filtered = filtered_reviews['sentiment'].mean() if not filtered_reviews.empty else 0; avg_all = sentiment_df['sentiment'].mean()
                    c1, c2 = st.columns(2); c1.metric(f"æ‰€é€‰è¯„è®º ({min_r}-{max_r} æ˜Ÿ) çš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_filtered:.2f}"); c2.metric("æ‰€æœ‰è¯„è®ºçš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_all:.2f}")
                    st.markdown("---"); st.subheader("ğŸ¤– AI è¯„è®ºæ·±åº¦åˆ†ææŠ¥å‘Š")
                    if st.button("ç”Ÿæˆ AI åˆ†ææŠ¥å‘Š", key="generate_review_report"):
                        if not (api_key_from_env := os.environ.get("ARK_API_KEY")): st.error("æ— æ³•æ‰¾åˆ° API Keyã€‚è¯·ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®ã€‚")
                        elif filtered_reviews.empty: st.warning("å½“å‰ç­›é€‰èŒƒå›´å†…æ²¡æœ‰è¯„è®ºå¯ä¾›AIåˆ†æï¼Œè¯·è°ƒæ•´ä¸Šæ–¹çš„æ˜Ÿçº§æ»‘å—ã€‚")
                        else:
                            expander = st.expander("æŸ¥çœ‹AIæ€è€ƒè¿‡ç¨‹", expanded=True); thinking_placeholder = expander.empty(); report_placeholder = st.empty()
                            full_response_text = ""; separator_think_end = "<<<<THINKING_ENDS>>>>"; separator_report_start = "<<<<REPORT_STARTS>>>>"
                            positive_samples = filtered_reviews.nlargest(15, 'sentiment'); negative_samples = filtered_reviews.nsmallest(15, 'sentiment')
                            positive_text = "\n".join([f"- {row['review_text']}" for _, row in positive_samples.iterrows()]); negative_text = "\n".join([f"- {row['review_text']}" for _, row in negative_samples.iterrows()])
                            try:
                                ark_client = Ark(api_key=api_key_from_env)
                                for chunk in generate_review_summary_report(ark_client, positive_text, negative_text):
                                    full_response_text += chunk
                                    if separator_report_start in full_response_text:
                                        parts1 = full_response_text.split(separator_think_end, 1); thinking_placeholder.markdown(parts1[0])
                                        report_part = parts1[1].split(separator_report_start, 1)[1]; report_placeholder.markdown(report_part)
                                    elif separator_think_end in full_response_text: thinking_placeholder.markdown(full_response_text.split(separator_think_end, 1)[0])
                                    else: thinking_placeholder.markdown(full_response_text + "...")
                            except Exception as e: st.error(f"ç”Ÿæˆè¯„è®ºåˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            else: st.info("è¯·ä¸Šä¼ è¯„è®ºæ–‡ä»¶ä»¥å¯ç”¨æ­¤åˆ†æã€‚")
    else: st.warning("è¯·ä¸Šä¼ æ‚¨çš„é”€å”®æ•°æ®ä»¥å¼€å§‹è‡ªæˆ‘éªŒè¯ã€‚")

def render_action_page():
    """æ¸²æŸ“ç¬¬ä¸‰æ­¥ï¼šè¡ŒåŠ¨æ–¹æ¡ˆé¡µé¢"""
    st.title("ç¬¬ä¸‰æ­¥ï¼šè¡ŒåŠ¨æ–¹æ¡ˆ (Action Plan)"); st.info("ç»“åˆAIçš„å¸‚åœºæ´å¯Ÿå’Œæ‚¨çš„å†…éƒ¨æ•°æ®éªŒè¯ï¼Œä¸€é”®ç”Ÿæˆä¸ºæ‚¨é‡èº«å®šåˆ¶çš„è¡ŒåŠ¨è®¡åˆ’ã€‚")
    if not st.session_state.market_report_content: st.warning("è¯·å…ˆåœ¨â€œæœºä¼šæ´å¯Ÿâ€æ­¥éª¤ç”Ÿæˆå¸‚åœºåˆ†ææŠ¥å‘Šã€‚"); return
    if not st.session_state.validation_summary: st.warning("è¯·å…ˆåœ¨â€œè‡ªæˆ‘éªŒè¯â€æ­¥éª¤ä¸Šä¼ æ•°æ®å¹¶å®Œæˆåˆ†æï¼Œä»¥æä¾›å†…éƒ¨æ•°æ®å‚è€ƒã€‚"); return
    st.subheader("å†³ç­–ä¾æ®é¢„è§ˆ")
    with st.expander("ç‚¹å‡»æŸ¥çœ‹AIçš„å¸‚åœºæ´å¯ŸæŠ¥å‘Š"): st.markdown(st.session_state.market_report_content)
    with st.expander("ç‚¹å‡»æŸ¥çœ‹æ‚¨çš„å†…éƒ¨æ•°æ®éªŒè¯æ‘˜è¦"): st.markdown(st.session_state.validation_summary)
    if not (api_key_from_env := os.environ.get("ARK_API_KEY")): st.error("æ— æ³•æ‰¾åˆ° API Keyã€‚è¯·ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®ã€‚"); return
    if st.button("ğŸ’¡ ç”Ÿæˆæˆ‘çš„è¡ŒåŠ¨è®¡åˆ’", type="primary", use_container_width=True):
        expander = st.expander("æŸ¥çœ‹AIæ€è€ƒè¿‡ç¨‹", expanded=True); thinking_placeholder = expander.empty(); report_placeholder = st.empty()
        full_response_text = ""; separator_think_end = "<<<<THINKING_ENDS>>>>"; separator_report_start = "<<<<REPORT_STARTS>>>>"
        try:
            ark_client = Ark(api_key=api_key_from_env)
            for chunk in agent_action_planner(ark_client, st.session_state.market_report_content, st.session_state.validation_summary):
                full_response_text += chunk
                if separator_report_start in full_response_text:
                    parts1 = full_response_text.split(separator_think_end, 1); thinking_placeholder.markdown(parts1[0])
                    report_part = parts1[1].split(separator_report_start, 1)[1]; report_placeholder.markdown(report_part)
                elif separator_think_end in full_response_text: thinking_placeholder.markdown(full_response_text.split(separator_think_end, 1)[0])
                else: thinking_placeholder.markdown(full_response_text + "...")
        except Exception as e: st.error(f"ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’æ—¶å‡ºé”™: {e}")

# ==============================================================================
# 6. ä¸»åº”ç”¨å¸ƒå±€ä¸é€»è¾‘
# ==============================================================================
st.set_page_config(layout="wide", page_title="WeaveAI æ™ºèƒ½åˆ†æåŠ©æ‰‹")
st.title('ğŸ“ˆ WeaveAI æ™ºèƒ½åˆ†æåŠ©æ‰‹ v2.8')
st.caption('##### å‘Šåˆ«æ„Ÿè§‰ï¼Œè®©æ•°æ®ä¸AIä¸ºæ‚¨å¼•èˆª')

initialize_session_state()

if not st.session_state.profile_created:
    render_profile_creation_page()
else:
    with st.sidebar:
        st.header(f"ğŸ§­ WeaveAI æˆ˜ç•¥å¯¼èˆª")
        with st.expander("ğŸ“ æˆ‘çš„æˆ˜ç•¥æ¡£æ¡ˆ", expanded=True):
            profile = st.session_state.profile
            st.markdown(f"**å•†ä¸šèº«ä»½:** {profile['seller_type']}\n\n**ç›®æ ‡å¸‚åœº:** {profile['target_market']}\n\n**æ ¸å¿ƒå“ç±»:** {profile['supply_chain']}\n\n**å®šä»·åŒºé—´:** ${profile['min_price']} - ${profile['max_price']}")
            if st.button("âœï¸ ä¿®æ”¹æ¡£æ¡ˆ"): st.session_state.profile_created = False; st.rerun()
        st.divider()
        st.session_state.current_step = st.radio("åˆ†ææµç¨‹", ['æœºä¼šæ´å¯Ÿ', 'è‡ªæˆ‘éªŒè¯', 'è¡ŒåŠ¨æ–¹æ¡ˆ'], captions=["AIé©±åŠ¨çš„å¸‚åœºæœºä¼šå‘ç°", "ç”¨æ‚¨çš„æ•°æ®éªŒè¯å¯è¡Œæ€§", "ç”Ÿæˆå¯æ‰§è¡Œçš„è¡ŒåŠ¨è®¡åˆ’"], key='navigation_radio')
    
    if st.session_state.current_step == 'æœºä¼šæ´å¯Ÿ': render_insight_page()
    elif st.session_state.current_step == 'è‡ªæˆ‘éªŒè¯': render_validation_page()
    elif st.session_state.current_step == 'è¡ŒåŠ¨æ–¹æ¡ˆ': render_action_page()