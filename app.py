# --- START OF FILE app.py ---

# ==============================================================================
# 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“
# ==============================================================================
import streamlit as st
import pandas as pd
import numpy as np
from stqdm import stqdm
import os
import warnings
import re 
import docx
import time

# AI Agent æ‰€éœ€çš„åº“
from dotenv import load_dotenv
from volcenginesdkarkruntime import Ark

# åœ¨ç¨‹åºæœ€å¼€å§‹åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# (å…³é”®) è§£å†³ KMeans å†…å­˜æ³„æ¼è­¦å‘Š
os.environ['OMP_NUM_THREADS'] = '1'

# åˆ†æåº“
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Input
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from pandas.errors import SettingWithCopyWarning, DtypeWarning

# æŠ‘åˆ¶ç‰¹å®šçš„Pandasè­¦å‘Šï¼Œç¾åŒ–è¾“å‡º
warnings.filterwarnings('ignore', category=SettingWithCopyWarning)
warnings.filterwarnings('ignore', category=DtypeWarning)

import plotly.graph_objects as go
import plotly.express as px

# ==============================================================================
# 2. é…ç½®ä¸è¾…åŠ©å‡½æ•°
# ==============================================================================

def generate_market_report(ark_client, target_market, use_web_search):
    """
    ç¬¬ä¸€è½®ï¼šAI å¸‚åœºåˆ†ææ¨¡å—
    """
    system_prompt = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è·¨å¢ƒç”µå•†å¸‚åœºåˆ†æå¸ˆä¸é€‰å“ä¸“å®¶ã€‚
    ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„å›½å®¶æˆ–åœ°åŒºï¼Œåˆ©ç”¨ä½ çš„çŸ¥è¯†å’Œç½‘ç»œæœç´¢èƒ½åŠ›ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€è¯¦å°½ã€ä¸”å…·æœ‰å‰ç»æ€§çš„é€‰å“åˆ†ææŠ¥å‘Šã€‚
    æŠ¥å‘Šå¿…é¡»ä½¿ç”¨Markdownæ ¼å¼ï¼Œç»“æ„æ¸…æ™°ï¼ŒåŒ…å«æ ‡é¢˜ã€åˆ—è¡¨å’ŒåŠ ç²—ç­‰å…ƒç´ ã€‚
    æŠ¥å‘Šå¿…é¡»éµå¾ªä»¥ä¸‹ç»“æ„å’Œè¦æ±‚ï¼š
    ## ä¸€ã€ å¸‚åœºæœºé‡æ¦‚è¦
    - æ€»ç»“æ ¸å¿ƒæœºé‡ï¼Œç›´æ¥ç‚¹å‡º2-3ä¸ªæ¨èäº§å“å¤§ç±»ã€‚
    ## äºŒã€ é‡ç‚¹é€‰å“å“ç±»æ·±åº¦åˆ†æ
    - å¯¹æ¯ä¸ªå¤§ç±»ï¼Œä»ä»¥ä¸‹å››ç»´åº¦åˆ†æï¼š
    1.  **å¸‚åœºæ½œåŠ›ä¸è¶‹åŠ¿**: ç»“åˆæ•°æ®è§£é‡Šå¢é•¿ç©ºé—´ã€‚
    2.  **æ–‡åŒ–ä¸æ¶ˆè´¹ä¹ æƒ¯å¥‘åˆåº¦**: ã€æ ¸å¿ƒã€‘åˆ†æä¸å½“åœ°æ–‡åŒ–ã€ç”Ÿæ´»æ–¹å¼çš„ç»“åˆç‚¹ã€‚
    3.  **æ³•å¾‹æ³•è§„ä¸å…³ç¨è€ƒé‡**: ã€æ ¸å¿ƒã€‘æ˜ç¡®æŒ‡å‡ºè¿›å£é™åˆ¶ã€æ‰€éœ€è®¤è¯å’Œå…³ç¨æ”¿ç­–ã€‚
    4.  **å…·ä½“é€‰å“å»ºè®®**: ç»™å‡º2-3ä¸ªå…·ä½“çš„ã€å¯æ‰§è¡Œçš„ç»†åˆ†äº§å“å»ºè®®ã€‚
    ## ä¸‰ã€ é£é™©ä¸æŒ‘æˆ˜
    - å®¢è§‚æŒ‡å‡ºç‰©æµã€æ”¯ä»˜ã€ç«äº‰ç­‰é£é™©ã€‚
    ## å››ã€ æ€»ç»“ä¸æœ€ç»ˆå»ºè®®
    - æ€»ç»“æŠ¥å‘Šï¼Œç»™å‡ºæ˜ç¡®çš„è¡ŒåŠ¨å»ºè®®ã€‚
    ä½ å¿…é¡»è°ƒç”¨`web_search`å·¥å…·ç¡®ä¿æ•°æ®æœ€æ–°ã€‚æŠ¥å‘Šè¯­è¨€åº”ä¸“ä¸šã€å®¢è§‚ã€‚
    """
    user_input = f"è¯·ä¸ºæˆ‘ç”Ÿæˆä¸€ä»½å…³äº'{target_market}'å¸‚åœºçš„è·¨å¢ƒç”µå•†é€‰å“åˆ†ææŠ¥å‘Šã€‚"
    
    request_params = {
        "model": "doubao-seed-1-6-250615",
        "input": [{"role": "system", "content": [{"type": "input_text", "text": system_prompt}]},
                  {"role": "user", "content": [{"type": "input_text", "text": user_input}]}],
        "stream": True,
        "extra_body": {"thinking": {"type": "auto"}},
    }
    if use_web_search:
        request_params["tools"] = [{"type": "web_search", "limit": 10}]

    try:
        response = ark_client.responses.create(**request_params)
        for chunk in response:
            delta_content = getattr(chunk, 'delta', None)
            if isinstance(delta_content, str): yield delta_content
    except Exception as e:
        yield f"\n\nâŒ **AI Agentè¯·æ±‚å¤±è´¥:**\n\n`{str(e)}`"

def generate_improvement_suggestions(ark_client, market_report, product_description, use_web_search):
    """
    ç¬¬äºŒè½®ï¼šAI äº§å“æ”¹è¿›å»ºè®®æ¨¡å—
    """
    system_prompt = """
    ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è·¨å¢ƒç”µå•†äº§å“æœ¬åœ°åŒ–æˆ˜ç•¥é¡¾é—®ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯æ·±å…¥åˆ†æä¸€ä»½å·²æœ‰çš„å¸‚åœºæŠ¥å‘Šå’Œä¸€ä»½ç”¨æˆ·æä¾›çš„äº§å“æè¿°ï¼Œç„¶åç»“åˆæœ€æ–°çš„ç½‘ç»œæœç´¢ä¿¡æ¯ï¼Œä¸ºè¯¥äº§å“è¿›å…¥ç›®æ ‡å¸‚åœºæä¾›ä¸€ä»½ä¸“ä¸šã€å…·ä½“ã€å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®æŠ¥å‘Šã€‚
    æŠ¥å‘Šå¿…é¡»ä½¿ç”¨Markdownæ ¼å¼ï¼Œå¹¶ä¸¥æ ¼éµå¾ªä»¥ä¸‹ç»“æ„ï¼š
    ### ğŸ“ äº§å“æ ¸å¿ƒç‰¹æ€§æ€»ç»“
    - é¦–å…ˆï¼Œç”¨ä¸€ä¸¤å¥è¯æ€»ç»“ä½ ç†è§£çš„è¿™æ¬¾äº§å“çš„æ ¸å¿ƒåŠŸèƒ½å’Œç›®æ ‡ç”¨æˆ·ã€‚
    ### ğŸš€ å¸‚åœºæœºé‡ç»“åˆç‚¹ (Opportunities)
    - æ˜ç¡®æŒ‡å‡ºè¿™æ¬¾äº§å“ä¸å¸‚åœºæŠ¥å‘Šä¸­æåˆ°çš„å“ªäº› **æœºé‡** å’Œ **æ–‡åŒ–æ¶ˆè´¹ä¹ æƒ¯** é«˜åº¦å¥‘åˆï¼Œè¿™æ˜¯äº§å“çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚
    ### âš ï¸ æ½œåœ¨é£é™©ä¸æŒ‘æˆ˜ (Challenges)
    - æ˜ç¡®æŒ‡å‡ºè¿™æ¬¾äº§å“å¯èƒ½ä¼šè§¦ç¢°åˆ°å¸‚åœºæŠ¥å‘Šä¸­æåˆ°çš„å“ªäº› **é£é™©**ã€**æ³•å¾‹æ³•è§„** æˆ– **è®¤è¯è¦æ±‚**ï¼Œè¿™æ˜¯éœ€è¦ä¼˜å…ˆè§£å†³çš„é—®é¢˜ã€‚
    ### ğŸ’¡ å…·ä½“æ”¹è¿›å»ºè®® (Actionable Suggestions)
    - è¿™æ˜¯æŠ¥å‘Šçš„æ ¸å¿ƒã€‚æä¾›ä¸€ä¸ªåŒ…å«å…·ä½“å»ºè®®çš„åˆ—è¡¨ï¼Œè‡³å°‘è¦†ç›–ä»¥ä¸‹3-4ä¸ªæ–¹é¢ï¼š
        - **åŠŸèƒ½ä¸è®¾è®¡è°ƒæ•´**: å»ºè®®å¯¹äº§å“çš„é¢œè‰²ã€å°ºå¯¸ã€åŠŸèƒ½ç‚¹ã€åŒ…è£…è®¾è®¡ç­‰è¿›è¡Œå“ªäº›è°ƒæ•´ä»¥æ›´ç¬¦åˆå½“åœ°å®¡ç¾å’Œä½¿ç”¨ä¹ æƒ¯ã€‚
        - **è¥é”€è¯­è¨€ä¸å–ç‚¹æç‚¼**: å»ºè®®åœ¨äº§å“è¯¦æƒ…é¡µå’Œå¹¿å‘Šä¸­ä½¿ç”¨å“ªäº›å…³é”®è¯å’Œè¥é”€è§’åº¦ï¼Œä»¥ç²¾å‡†è§¦è¾¾å½“åœ°æ¶ˆè´¹è€…ã€‚
        - **å®šä»·ä¸æœåŠ¡ç­–ç•¥**: å»ºè®®ä¸€ä¸ªåˆæ­¥çš„å®šä»·åŒºé—´ï¼Œå¹¶æŒ‡å‡ºæ˜¯å¦éœ€è¦æä¾›ç‰¹æ®Šçš„æ”¯ä»˜æ–¹å¼ï¼ˆå¦‚CODï¼‰æˆ–å”®åæœåŠ¡ã€‚
        - **åˆè§„æ€§æ£€æŸ¥**: æé†’ç”¨æˆ·éœ€è¦æ£€æŸ¥æˆ–è·å–å“ªäº›å…·ä½“çš„è®¤è¯æˆ–è®¸å¯ä»¥åŠæ—©å‡†å¤‡ã€‚
    """
    
    user_input = f"""
    è¿™æ˜¯æˆ‘ä¹‹å‰ç”Ÿæˆçš„å¸‚åœºåˆ†ææŠ¥å‘Šï¼š
    --- [å¸‚åœºæŠ¥å‘Šå¼€å§‹] ---
    {market_report}
    --- [å¸‚åœºæŠ¥å‘Šç»“æŸ] ---

    è¿™æ˜¯æˆ‘çš„äº§å“ä»‹ç»ï¼š
    --- [äº§å“ä»‹ç»å¼€å§‹] ---
    {product_description}
    --- [äº§å“ä»‹ç»ç»“æŸ] ---

    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºæˆ‘çš„äº§å“ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æœ¬åœ°åŒ–æ”¹è¿›å»ºè®®æŠ¥å‘Šã€‚
    """

    request_params = {
        "model": "doubao-seed-1-6-250615",
        "input": [{"role": "system", "content": [{"type": "input_text", "text": system_prompt}]},
                  {"role": "user", "content": [{"type": "input_text", "text": user_input}]}],
        "stream": True,
        "extra_body": {"thinking": {"type": "auto"}},
    }
    if use_web_search:
        request_params["tools"] = [{"type": "web_search", "limit": 5}]

    try:
        response = ark_client.responses.create(**request_params)
        for chunk in response:
            delta_content = getattr(chunk, 'delta', None)
            if isinstance(delta_content, str): yield delta_content
    except Exception as e:
        yield f"\n\nâŒ **AI Agentè¯·æ±‚å¤±è´¥:**\n\n`{str(e)}`"

def generate_review_summary_report(ark_client, positive_reviews_sample, negative_reviews_sample):
    """
    æ–°å¢ï¼šAI è¯„è®ºåˆ†ææ¨¡å—
    """
    system_prompt = """
    ä½ æ˜¯ä¸€ä½é«˜çº§ç”¨æˆ·æ´å¯Ÿåˆ†æå¸ˆï¼Œä¸“æ³¨äºä»å¤§é‡ç”¨æˆ·è¯„è®ºä¸­æç‚¼æ ¸å¿ƒè§‚ç‚¹å’Œå•†ä¸šæ´è§ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç»™ä½ çš„æ­£é¢å’Œè´Ÿé¢è¯„è®ºæ ·æœ¬ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ç®€æ´ã€æ·±åˆ»ã€ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚
    æŠ¥å‘Šå¿…é¡»ä½¿ç”¨Markdownæ ¼å¼ï¼Œå¹¶ä¸¥æ ¼éµå¾ªä»¥ä¸‹ç»“æ„ï¼š
    ### ğŸ“ è¯„è®ºæ€»ä½“æƒ…ç»ªæ¦‚è¿°
    - ç”¨ä¸€ä¸¤å¥è¯ï¼ŒåŸºäºä½ çœ‹åˆ°çš„è¯„è®ºï¼Œæ€»ç»“äº§å“çš„æ•´ä½“å¸‚åœºåå“å’Œç”¨æˆ·æƒ…ç»ªã€‚
    ### ğŸ‘ äº§å“æ ¸å¿ƒä¼˜åŠ¿ (ä»æ­£é¢è¯„è®ºä¸­æç‚¼)
    - ä½¿ç”¨åˆ—è¡¨å½¢å¼ï¼Œæ€»ç»“å‡ºç”¨æˆ·æœ€å¸¸ç§°èµçš„2-3ä¸ªæ ¸å¿ƒä¼˜ç‚¹ã€‚
    - æ¯ä¸€ä¸ªä¼˜ç‚¹åé¢ï¼Œç”¨æ‹¬å·å¼•ç”¨ä¸€å¥æœ€èƒ½ä»£è¡¨è¯¥è§‚ç‚¹çš„ **åŸå§‹è¯„è®º**ã€‚
    ### ğŸ‘ äº§å“ä¸»è¦ç—›ç‚¹ (ä»è´Ÿé¢è¯„è®ºä¸­æç‚¼)
    - ä½¿ç”¨åˆ—è¡¨å½¢å¼ï¼Œæ€»ç»“å‡ºç”¨æˆ·æŠ±æ€¨æœ€å¤šçš„2-3ä¸ªæ ¸å¿ƒé—®é¢˜æˆ–ç¼ºç‚¹ã€‚
    - æ¯ä¸€ä¸ªç—›ç‚¹åé¢ï¼Œç”¨æ‹¬å·å¼•ç”¨ä¸€å¥æœ€èƒ½ä»£è¡¨è¯¥è§‚ç‚¹çš„ **åŸå§‹è¯„è®º**ã€‚
    ### ğŸ’¡ å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®® (Actionable Suggestions)
    - åŸºäºä»¥ä¸Šåˆ†æï¼Œä¸ºäº§å“ç»ç†æˆ–è¿è¥å›¢é˜Ÿæä¾›2-3æ¡å…·ä½“çš„ã€å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®ã€‚
    """
    
    user_input = f"""
    ä»¥ä¸‹æ˜¯å…³äºæŸæ¬¾äº§å“çš„ç”¨æˆ·è¯„è®ºæ ·æœ¬ã€‚
    --- [æ­£é¢è¯„è®ºæ ·æœ¬] ---
    {positive_reviews_sample}
    --- [æ­£é¢è¯„è®ºæ ·æœ¬ç»“æŸ] ---
    --- [è´Ÿé¢è¯„è®ºæ ·æœ¬] ---
    {negative_reviews_sample}
    --- [è´Ÿé¢è¯„è®ºæ ·æœ¬ç»“æŸ] ---
    è¯·æ ¹æ®ä»¥ä¸Šè¯„è®ºï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½ç”¨æˆ·æ´å¯Ÿåˆ†ææŠ¥å‘Šã€‚
    """

    # --- ã€ä»£ç é£æ ¼ç»Ÿä¸€ä¿®æ­£ã€‘ ---
    request_params = {
        "model": "doubao-seed-1-6-250615",
        "input": [
            {"role": "system", "content": [{"type": "input_text", "text": system_prompt}]},
            {"role": "user", "content": [{"type": "input_text", "text": user_input}]}
        ],
        "stream": True,
        "extra_body": {"thinking": {"type": "auto"}},
    }

    try:
        response = ark_client.responses.create(**request_params)
        for chunk in response:
            delta_content = getattr(chunk, 'delta', None)
            if isinstance(delta_content, str):
                yield delta_content
    except Exception as e:
        yield f"\n\nâŒ **AI Agentè¯·æ±‚å¤±è´¥:**\n\n`{str(e)}`"

@st.cache_data
def convert_df_to_csv(df): return df.to_csv(index=False).encode('utf-8-sig')

def load_uploaded_file(uploaded_file, dtype_spec=None):
    if uploaded_file is None: return None;
    try:
        file_name = uploaded_file.name
        if file_name.endswith('.parquet'): return pd.read_parquet(uploaded_file)
        elif file_name.endswith('.csv'): return pd.read_csv(uploaded_file, on_bad_lines='skip', dtype=dtype_spec)
        else: st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_name}ã€‚"); return None
    except Exception as e: st.error(f"è¯»å–æ–‡ä»¶ '{uploaded_file.name}' æ—¶å‡ºé”™: {e}"); return None

@st.cache_data
def perform_lstm_forecast(_df):
    sales_ts = _df.groupby('Date')['Amount'].sum().asfreq('D', fill_value=0); sales_values = sales_ts.values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1)); scaled_values = scaler.fit_transform(sales_values)
    def create_dataset(data, look_back=7):
        X, y = [], [];
        for i in range(len(data) - look_back): X.append(data[i:(i + look_back), 0]); y.append(data[i + look_back, 0])
        return np.array(X), np.array(y)
    look_back = 7; X, y = create_dataset(scaled_values, look_back); X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    model = Sequential([Input(shape=(look_back, 1)), LSTM(50), Dense(1)]); model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X, y, epochs=20, batch_size=32, verbose=0)
    last_days_scaled = scaled_values[-look_back:]; current_input = np.reshape(last_days_scaled, (1, look_back, 1)); future_predictions_scaled = []
    for _ in range(30):
        next_pred_scaled = model.predict(current_input, verbose=0); future_predictions_scaled.append(next_pred_scaled[0, 0])
        new_pred_reshaped = np.reshape(next_pred_scaled, (1, 1, 1)); current_input = np.append(current_input[:, 1:, :], new_pred_reshaped, axis=1)
    future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))
    last_date = sales_ts.index[-1]; future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30); fig = go.Figure()
    fig.add_trace(go.Scatter(x=sales_ts.index, y=sales_ts.values, name='å†å²é”€å”®é¢', line=dict(color='royalblue', width=2), fill='tozeroy', fillcolor='rgba(65, 105, 225, 0.2)'))
    fig.add_trace(go.Scatter(x=future_dates, y=future_predictions.flatten(), name='LSTM é¢„æµ‹é”€å”®é¢', line=dict(color='darkorange', dash='dash', width=2), fill='tozeroy', fillcolor='rgba(255, 140, 0, 0.2)'))
    fig.update_layout(title='æœªæ¥30å¤©é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTMæ¨¡å‹)', xaxis_title='æ—¥æœŸ', yaxis_title='é”€å”®é¢'); return fig

@st.cache_data
def perform_product_clustering(_df):
    required_cols = ['SKU', 'Amount', 'Qty', 'Order ID'];
    if not all(col in _df.columns for col in _df.columns): return None, None, f"èšç±»åˆ†æå¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„åˆ—ã€‚"
    product_agg_df = _df.groupby('SKU').agg(total_amount=('Amount', 'sum'), total_qty=('Qty', 'sum'), order_count=('Order ID', 'nunique')).reset_index()
    features = product_agg_df[['total_amount', 'total_qty', 'order_count']]; scaler = StandardScaler(); features_scaled = scaler.fit_transform(features)
    kmeans = KMeans(n_clusters=3, n_init=10, random_state=42); product_agg_df['cluster'] = kmeans.fit_predict(features_scaled)
    cluster_summary = product_agg_df.groupby('cluster')[['total_amount', 'total_qty', 'order_count']].mean().sort_values(by='total_amount', ascending=False)
    hot_product_cluster_id = cluster_summary.index[0]
    hot_products = product_agg_df[product_agg_df['cluster'] == hot_product_cluster_id].sort_values(by='total_amount', ascending=False)
    return cluster_summary, hot_products, None

def find_review_column(df):
    priority_cols = ['reviews.text', 'review_text', 'content', 'comment', 'review']
    for p_col in priority_cols:
        if p_col in df.columns and df[p_col].dropna().astype(str).str.strip().any(): return p_col
    possible_cols = [col for col in df.columns if any(key in str(col).lower() for key in ['text', 'review', 'content', 'comment'])]
    if possible_cols:
        string_cols = [col for col in possible_cols if df[col].dtype == 'object'];
        if string_cols: return max(string_cols, key=lambda col: df[col].dropna().astype(str).str.len().mean())
    object_cols = df.select_dtypes(include=['object']).columns
    if not object_cols.empty:
        for col in object_cols:
            if df[col].dropna().astype(str).str.strip().any(): return col
    return None
    
@st.cache_data
def perform_sentiment_analysis(reviews_df):
    def sentiment_to_rating(sentiment):
        if sentiment >= 0.5: return 5
        elif sentiment >= 0.05: return 4
        elif sentiment > -0.05: return 3
        elif sentiment > -0.5: return 2
        else: return 1
    analyzer = SentimentIntensityAnalyzer(); review_column_name = find_review_column(reviews_df);
    if review_column_name is None: return None, "é”™è¯¯: æœªèƒ½åœ¨è¯„è®ºæ–‡ä»¶ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬åˆ—ã€‚"
    reviews_df[review_column_name] = reviews_df[review_column_name].astype(str).dropna()
    reviews_df = reviews_df[reviews_df[review_column_name].str.strip() != 'None'].copy()
    stqdm.pandas(desc="æ­£åœ¨è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"); reviews_df['sentiment'] = reviews_df[review_column_name].progress_apply(lambda text: analyzer.polarity_scores(text)['compound'])
    if 'rating' not in reviews_df.columns: reviews_df['rating'] = reviews_df['sentiment'].apply(sentiment_to_rating)
    reviews_df.rename(columns={review_column_name: 'review_text'}, inplace=True); return reviews_df, None

@st.cache_data
def create_category_sales_plot(_df):
    category_means = _df.groupby('Category')['Amount'].mean().sort_values(ascending=False).reset_index()
    fig = px.bar(category_means, x='Category', y='Amount', color='Category', text_auto='.2f', labels={'Category': 'äº§å“ç±»åˆ«', 'Amount': 'å¹³å‡é”€å”®é¢ (Amount)'}, title='å„äº§å“ç±»åˆ«å¹³å‡é”€å”®é¢å¯¹æ¯”')
    fig.update_layout(width=800, height=500, template='plotly_white', showlegend=False)
    fig.update_traces(textposition='outside')
    return fig

# ==============================================================================
# 3. Streamlit ç”¨æˆ·ç•Œé¢å¸ƒå±€
# ==============================================================================
st.set_page_config(layout="wide"); st.title('ğŸ“ˆ WeaveAIæ™ºèƒ½åˆ†æåŠ©æ‰‹')
st.caption('##### è¿˜åœ¨å‡­æ„Ÿè§‰é€‰å“ï¼Ÿè®©æ•°æ®ä¸AIä¸ºæ‚¨å¼•èˆª')

with st.sidebar:
    st.header("ğŸ“‚ ä¸Šä¼ æ‚¨çš„æ•°æ®"); st.info("æ¨èæ‚¨å°†å¤§çš„CSVæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼ä»¥æå‡é€Ÿåº¦ã€‚")
    uploaded_amazon = st.file_uploader('1. ä¸Šä¼  Amazon é”€å”®æŠ¥å‘Š', type=['csv', 'parquet'])
    uploaded_reviews = st.file_uploader('2. ä¸Šä¼  Amazon è¯„è®ºæ•°æ® (å¯é€‰)', type=['csv', 'parquet'])

if 'report_generated' not in st.session_state: st.session_state.report_generated = False
if 'market_report_content' not in st.session_state: st.session_state.market_report_content = ""

tabs = ["ğŸ¤– AI å¸‚åœºé€‰å“é¡¾é—®", "ğŸ§  LSTMé”€å”®é¢„æµ‹", "ğŸ›ï¸ å“ç±»è¡¨ç°", "ğŸ”¥ çƒ­é”€å“èšç±»", "ğŸ’¬ æƒ…æ„Ÿåˆ†æ"]
tab_agent, tab_lstm, tab_category, tab_cluster, tab_sentiment = st.tabs(tabs)

with tab_agent:
    st.header("ç¬¬ä¸€è½®ï¼šAI å¸‚åœºé€‰å“é¡¾é—®"); 
    col1, col2 = st.columns([3, 2])
    with col1:
        target_market = st.text_input("è¾“å…¥æ‚¨æƒ³åˆ†æçš„ç›®æ ‡å›½å®¶æˆ–åœ°åŒº", placeholder="ä¾‹å¦‚ï¼šå¾·å›½ã€ä¸œå—äºšã€å·´è¥¿")
    with col2:
        use_weaveai_db = st.toggle("è°ƒç”¨'WeaveAI'å®æ—¶æ•°æ®åº“", value=True, help="å¼€å¯åï¼ŒAIå°†ä½¿ç”¨å®æ—¶æ›´æ–°çš„æœ€æ–°æ•°æ®åº“ä»¥æä¾›æ›´å…·æ—¶æ•ˆæ€§çš„åˆ†æã€‚å…³é—­åˆ™ä»…ä¾èµ–æ¨¡å‹è‡ªèº«çŸ¥è¯†ã€‚")

    if st.button("ç”Ÿæˆåˆ†ææŠ¥å‘Š", key="generate_report"):
        st.session_state.report_generated = False; st.session_state.market_report_content = ""
        api_key_from_env = os.environ.get("ARK_API_KEY")
        if not api_key_from_env: st.error("æ— æ³•æ‰¾åˆ° API Keyã€‚è¯·ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®ã€‚")
        elif not target_market.strip(): st.warning("è¯·è¾“å…¥ç›®æ ‡å›½å®¶æˆ–åœ°åŒºã€‚")
        else:
            st.markdown("---"); expander = st.expander("ç‚¹å‡»æŸ¥çœ‹AIçš„æ€è€ƒè¿‡ç¨‹ ğŸ§ ", expanded=False); thinking_placeholder = expander.empty()
            st.markdown(f"### ğŸŒ {target_market}å¸‚åœºè·¨å¢ƒç”µå•†é€‰å“åˆ†ææŠ¥å‘Š"); report_placeholder = st.empty()
            full_response_text = ""; separator_pattern = re.compile(r"\n#+ ")
            try:
                ark_client = Ark(api_key=api_key_from_env)
                for chunk in generate_market_report(ark_client, target_market, use_weaveai_db):
                    full_response_text += chunk; match = separator_pattern.search(full_response_text)
                    if match:
                        thinking_part = full_response_text[:match.start()]; report_part = full_response_text[match.start():]
                        thinking_placeholder.markdown(thinking_part); report_placeholder.markdown(report_part)
                    else:
                        thinking_placeholder.markdown(full_response_text)
                match = separator_pattern.search(full_response_text)
                if match: st.session_state.market_report_content = full_response_text[match.start():]
                else: st.session_state.market_report_content = full_response_text
                st.session_state.report_generated = True
            except Exception as e: st.error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–æˆ–è¯·æ±‚å¤±è´¥: {e}")

    if st.session_state.report_generated:
        st.markdown("---"); st.header("ç¬¬äºŒè½®ï¼šäº§å“æœ¬åœ°åŒ–æ”¹è¿›å»ºè®®")
        st.info("æ‚¨å·²äº†è§£å¸‚åœºæ¦‚å†µï¼Œç°åœ¨å¯ä»¥ä¸Šä¼ æ‚¨çš„äº§å“ä»‹ç»ï¼ˆ.txt, .md, .docxï¼‰ï¼ŒAIå°†ç»“åˆå¸‚åœºæŠ¥å‘Šä¸ºæ‚¨æä¾›æ”¹è¿›å»ºè®®ã€‚")
        uploaded_product_doc = st.file_uploader("ä¸Šä¼ æ‚¨çš„äº§å“ä»‹ç»æ–‡æ¡£", type=['txt', 'md', 'docx'])
        if uploaded_product_doc is not None:
            if st.button("è·å–æ”¹è¿›å»ºè®®", key="generate_suggestions"):
                api_key_from_env = os.environ.get("ARK_API_KEY")
                if not api_key_from_env: st.error("æ— æ³•æ‰¾åˆ° API Key ç”¨äºç¬¬äºŒè½®åˆ†æã€‚")
                else:
                    with st.spinner("ğŸš€ AIæ­£åœ¨ç»“åˆå¸‚åœºæŠ¥å‘Šå’Œæ‚¨çš„äº§å“ä¿¡æ¯ï¼Œç”Ÿæˆæ”¹è¿›å»ºè®®..."):
                        product_description = "";
                        try:
                            if uploaded_product_doc.name.endswith('.docx'):
                                doc = docx.Document(uploaded_product_doc); product_description = '\n'.join([para.text for para in doc.paragraphs])
                            else: product_description = uploaded_product_doc.read().decode("utf-8")
                        except Exception as e: st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                        if product_description:
                            st.markdown("---"); sugg_expander = st.expander("ç‚¹å‡»æŸ¥çœ‹ç¬¬äºŒè½®AIçš„æ€è€ƒè¿‡ç¨‹ ğŸ§ ", expanded=False)
                            sugg_thinking_placeholder = sugg_expander.empty(); st.markdown("#### ğŸ’¡ AI äº§å“æ”¹è¿›å»ºè®®"); sugg_report_placeholder = st.empty()
                            sugg_full_response_text = ""; sugg_separator_pattern = re.compile(r"\n#+ ")
                            try:
                                ark_client = Ark(api_key=api_key_from_env)
                                for chunk in generate_improvement_suggestions(ark_client, st.session_state.market_report_content, product_description, use_weaveai_db):
                                    sugg_full_response_text += chunk; sugg_match = sugg_separator_pattern.search(sugg_full_response_text)
                                    if sugg_match:
                                        sugg_thinking_part = sugg_full_response_text[:sugg_match.start()]; sugg_report_part = sugg_full_response_text[sugg_match.start():]
                                        sugg_thinking_placeholder.markdown(sugg_thinking_part); sugg_report_placeholder.markdown(sugg_report_part)
                                    else:
                                        sugg_thinking_placeholder.markdown(sugg_full_response_text)
                            except Exception as e: st.error(f"ç”Ÿæˆæ”¹è¿›å»ºè®®æ—¶å‡ºé”™: {e}")

data_analysis_container = st.container()
with data_analysis_container:
    if uploaded_amazon:
        dtype_spec = {'ASIN': str, 'asin': str}; amazon_df = load_uploaded_file(uploaded_amazon, dtype_spec=dtype_spec)
        reviews_df_loaded = load_uploaded_file(uploaded_reviews)
        if amazon_df is not None:
            with st.status("âš™ï¸ æ­£åœ¨æ¸…æ´—å’Œé€‚é…æ‚¨çš„é”€å”®æ•°æ®...", expanded=True) as status:
                for old, new in {'Total Sales':'Amount','Product':'SKU','Quantity':'Qty','Order_ID':'Order ID'}.items():
                    if old in amazon_df.columns: amazon_df.rename(columns={old:new}, inplace=True)
                req_cols = ["Amount","Category","Date","Status","SKU","Order ID","Qty"]; missing = [c for c in req_cols if c not in amazon_df.columns]
                if missing: status.update(label="æ•°æ®æ¸…æ´—å¤±è´¥!", state="error"); st.error(f"Amazon æ–‡ä»¶ä¸­ç¼ºå°‘å…³é”®åˆ—: {', '.join(missing)}")
                else:
                    amazon_df.dropna(subset=["Amount", "Category", "Date"], inplace=True)
                    try: amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], format='%m-%d-%y')
                    except ValueError: amazon_df["Date"] = pd.to_datetime(amazon_df["Date"], errors='coerce')
                    amazon_df["Amount"] = pd.to_numeric(amazon_df["Amount"], errors='coerce')
                    amazon_df = amazon_df[amazon_df["Status"].isin(["Shipped","Shipped - Delivered to Buyer","Completed","Pending","Cancelled"])]
                    amazon_df.dropna(subset=['Date','Amount','SKU','Order ID','Qty'], inplace=True)
                    status.update(label="æ•°æ®æ¸…æ´—ä¸é€‚é…å®Œæˆ!", state="complete", expanded=False)
                    st.write("â³ æ­£åœ¨ä¸ºæ‚¨è®¡ç®—å†å²é”€å”®æ•°æ®åˆ†ææ¨¡å‹...");
                    total_steps = 2 if reviews_df_loaded is not None else 1
                    progress_bar = st.progress(0, text=f"å¼€å§‹è®¡ç®—... (0/{total_steps})")
                    cluster_summary, hot_products, cluster_error = perform_product_clustering(amazon_df)
                    progress_bar.progress(100 if total_steps == 1 else 50, text=f"äº§å“èšç±»åˆ†æå®Œæˆ... (1/{total_steps})")
                    sentiment_df, sentiment_error = (None, None)
                    if reviews_df_loaded is not None:
                        sentiment_df, sentiment_error = perform_sentiment_analysis(reviews_df_loaded)
                        progress_bar.progress(100, text=f"æƒ…æ„Ÿåˆ†æå®Œæˆ... (2/{total_steps})")
                    time.sleep(0.5); progress_bar.empty()
                    st.success("âœ… æ ¸å¿ƒè®¡ç®—å®Œæˆï¼ç°åœ¨æ‚¨å¯ä»¥æµè§ˆæ‰€æœ‰åˆ†æç»“æœã€‚")
                    with tab_lstm: st.header("é”€å”®é¢æ·±åº¦å­¦ä¹ é¢„æµ‹ (LSTM)"); lstm_fig = perform_lstm_forecast(amazon_df); st.plotly_chart(lstm_fig, use_container_width=True)
                    with tab_category: st.header("äº§å“ç±»åˆ«é”€å”®è¡¨ç°"); cat_fig = create_category_sales_plot(amazon_df); st.plotly_chart(cat_fig)
                    with tab_cluster:
                        st.header("çƒ­é”€å•†å“èšç±»åˆ†æ")
                        if cluster_error: st.error(cluster_error)
                        elif cluster_summary is not None and hot_products is not None:
                            st.subheader("å„å•†å“ç°‡ç‰¹å¾å‡å€¼"); st.dataframe(cluster_summary)
                            st.subheader(f"ğŸ”¥ çƒ­é”€å•†å“åˆ—è¡¨ (å…± {len(hot_products)} ä¸ª)"); st.dataframe(hot_products.head(20));
                            if st.checkbox('æ˜¾ç¤ºæ‰€æœ‰çƒ­é”€å•†å“'): st.dataframe(hot_products)
                            st.download_button("ä¸‹è½½çƒ­é”€å•†å“åˆ—è¡¨ (CSV)", convert_df_to_csv(hot_products), "hot_products.csv")
                    with tab_sentiment:
                        st.header("å®¢æˆ·è¯„è®ºæƒ…æ„Ÿåˆ†æ")
                        if uploaded_reviews:
                            if sentiment_error: st.error(sentiment_error)
                            elif sentiment_df is not None:
                                st.subheader("æŒ‰æ˜Ÿçº§ç­›é€‰è¯„è®º"); rating_range = st.slider('é€‰æ‹©æ˜Ÿçº§èŒƒå›´:',1,5,(4,5)); min_r, max_r = rating_range
                                filtered_reviews = sentiment_df[(sentiment_df['rating']>=min_r)&(sentiment_df['rating']<=max_r)]
                                st.markdown(f"**æ˜¾ç¤º {len(filtered_reviews)} æ¡è¯„åˆ†ä¸º {min_r} åˆ° {max_r} æ˜Ÿçš„è¯„è®º**"); st.dataframe(filtered_reviews[['rating','review_text','sentiment']])
                                st.subheader("æƒ…æ„Ÿåˆ†æ•°ç»Ÿè®¡"); avg_filtered = filtered_reviews['sentiment'].mean() if not filtered_reviews.empty else 0; avg_all = sentiment_df['sentiment'].mean()
                                c1,c2 = st.columns(2); c1.metric(f"æ‰€é€‰è¯„è®º ({min_r}-{max_r} æ˜Ÿ) çš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_filtered:.2f}"); c2.metric("æ‰€æœ‰è¯„è®ºçš„å¹³å‡æƒ…æ„Ÿåˆ†", f"{avg_all:.2f}")
                                
                                st.markdown("---")
                                st.subheader("ğŸ¤– AI è¯„è®ºæ·±åº¦åˆ†ææŠ¥å‘Š")
                                if st.button("ç”Ÿæˆ AI åˆ†ææŠ¥å‘Š", key="generate_review_report"):
                                    api_key_from_env = os.environ.get("ARK_API_KEY")
                                    if not api_key_from_env: st.error("æ— æ³•æ‰¾åˆ° API Keyã€‚è¯·ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®ã€‚")
                                    elif filtered_reviews.empty: st.warning("å½“å‰ç­›é€‰èŒƒå›´å†…æ²¡æœ‰è¯„è®ºå¯ä¾›AIåˆ†æï¼Œè¯·è°ƒæ•´ä¸Šæ–¹çš„æ˜Ÿçº§æ»‘å—ã€‚")
                                    else:
                                        with st.spinner("ğŸš€ æ­£åœ¨ä¸ºæ‚¨æŠ½æ ·è¯„è®ºå¹¶è°ƒç”¨AIè¿›è¡Œåˆ†æ..."):
                                            positive_samples = filtered_reviews.nlargest(20, 'sentiment')
                                            negative_samples = filtered_reviews.nsmallest(20, 'sentiment')
                                            positive_text = "\n".join([f"- {row['review_text']}" for index, row in positive_samples.iterrows()])
                                            negative_text = "\n".join([f"- {row['review_text']}" for index, row in negative_samples.iterrows()])
                                            
                                            rev_expander = st.expander("ç‚¹å‡»æŸ¥çœ‹AIçš„æ€è€ƒè¿‡ç¨‹ ğŸ§ ", expanded=False)
                                            rev_thinking_placeholder = rev_expander.empty(); rev_report_placeholder = st.empty()
                                            rev_full_response_text = ""; rev_separator_pattern = re.compile(r"\n#+ ")
                                            try:
                                                ark_client = Ark(api_key=api_key_from_env)
                                                for chunk in generate_review_summary_report(ark_client, positive_text, negative_text):
                                                    rev_full_response_text += chunk
                                                    rev_match = rev_separator_pattern.search(rev_full_response_text)
                                                    if rev_match:
                                                        rev_thinking_part = rev_full_response_text[:rev_match.start()]
                                                        rev_report_part = rev_full_response_text[rev_match.start():]
                                                        rev_thinking_placeholder.markdown(rev_thinking_part)
                                                        rev_report_placeholder.markdown(rev_report_part)
                                                    else:
                                                        rev_thinking_placeholder.markdown(rev_full_response_text)
                                            except Exception as e: st.error(f"ç”Ÿæˆè¯„è®ºåˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {e}")
                        else: st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ è¯„è®ºæ–‡ä»¶ä»¥å¯ç”¨æ­¤åˆ†æã€‚")
    else:
        with tab_lstm: st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ Amazon é”€å”®æŠ¥å‘Šä»¥å¯ç”¨æ•°æ®åˆ†æåŠŸèƒ½ã€‚")
        with tab_category: st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ Amazon é”€å”®æŠ¥å‘Šä»¥å¯ç”¨æ•°æ®åˆ†æåŠŸèƒ½ã€‚")
        with tab_cluster: st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ Amazon é”€å”®æŠ¥å‘Šä»¥å¯ç”¨æ•°æ®åˆ†æåŠŸèƒ½ã€‚")
        with tab_sentiment: st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„ Amazon é”€å”®æŠ¥å‘Šå’Œè¯„è®ºæ–‡ä»¶ä»¥å¯ç”¨æ•°æ®åˆ†æåŠŸèƒ½ã€‚")